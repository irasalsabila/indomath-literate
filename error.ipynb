{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import openai\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder where the evaluation results are located\n",
    "folder = f\"mgsm/results_gemma\"  # <-- You can change this to point to the specific folder you want\n",
    "df = pd.read_csv(f\"{folder}/Xfinal_result.csv\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "display(df[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the languages you want to analyze\n",
    "# lang_order = ['id', 'jv', 'su']\n",
    "lang_order = ['bn', 'en', 'es', 'ja', 'te', 'th']\n",
    "\n",
    "# Find mistakes (incorrect answers based on manual review 'is_correct_rev2')\n",
    "mistake_rows = df[df['is_correct_rev2'] == False]\n",
    "\n",
    "# Group mistakes by language and question number\n",
    "lang_qno_mistakes = mistake_rows.groupby(['lang', 'q_no']).size().reset_index(name='mistake_count')\n",
    "\n",
    "# Find question numbers with the most mistakes per language\n",
    "most_mistakes_per_lang = (\n",
    "    lang_qno_mistakes\n",
    "    .sort_values(['lang', 'mistake_count'], ascending=[True, False])\n",
    "    .groupby('lang')['q_no']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Create a complete result dictionary for the specified language order\n",
    "complete_result = {lang: most_mistakes_per_lang.get(lang, []) for lang in lang_order}\n",
    "\n",
    "# Print mistakes per language\n",
    "for lang, q_nos in complete_result.items():\n",
    "    print(f\"{lang}: {q_nos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all mistake question numbers\n",
    "all_mistakes = [q_no for qnos in complete_result.values() for q_no in qnos]\n",
    "\n",
    "# Count how often each question number appears among mistakes\n",
    "qno_counts = Counter(all_mistakes)\n",
    "\n",
    "# Print the most commonly mistaken questions across all languages\n",
    "most_common = qno_counts.most_common()\n",
    "print(\"Question numbers with most mistakes across all languages:\")\n",
    "for qno, count in most_common:\n",
    "    print(f\"q_no {qno} â€” {count} mistake(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI API key\n",
    "api_key_file = 'apis/api.txt'\n",
    "with open(api_key_file, \"r\") as file:\n",
    "    api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_error(error_text, api_key, model_name=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Classify the type of error in a generated answer using OpenAI's GPT model.\n",
    "    \n",
    "    Args:\n",
    "        error_text (str): The explanation or reasoning text to classify.\n",
    "        api_key (str): OpenAI API key.\n",
    "        model_name (str): The model name to use (default: 'gpt-4o').\n",
    "        \n",
    "    Returns:\n",
    "        str: Predicted error type (\"Arithmetic Mistake\", \"Missing Calculation\", or \"Hallucination\").\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI(api_key=api_key) \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Classify the following error into one of these categories:\n",
    "\n",
    "1. Arithmetic Mistake: Miscalculation or flawed numerical logic.\n",
    "   - Incorrect math operation or formula.\n",
    "   - Misuse of units, percentages, or order of operations.\n",
    "   - Logical errors that lead to wrong numeric results.\n",
    "\n",
    "2. Missing Calculation: Incomplete reasoning or skipped steps.\n",
    "   - Skipping necessary steps in the solution.\n",
    "   - Leaving the final answer unfinished or abruptly ending.\n",
    "   - Ignoring parts of the question or data.\n",
    "\n",
    "3. Hallucination: Introducing unsupported or fabricated information.\n",
    "   - Adding assumptions or data not present in the prompt.\n",
    "   - Inventing variables, conditions, or steps.\n",
    "   - Confidently stating something untrue or unrelated.\n",
    "\n",
    "Error to classify: \"{error_text}\"\n",
    "\n",
    "Return only the category name: \"Arithmetic Mistake\", \"Missing Calculation\", or \"Hallucination\".\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in classifying math errors.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR:\", e)\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only incorrect rows for error classification\n",
    "incorrect_df = df[df[\"is_correct_rev2\"] == False].copy()\n",
    "print(len(incorrect_df))\n",
    "display(incorrect_df[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the error classification function to each incorrect answer\n",
    "incorrect_df[\"ec_1\"] = incorrect_df[\"self_revision\"].apply(lambda x: classify_error(x, api_key))\n",
    "\n",
    "# Display the result with relevant columns\n",
    "display(incorrect_df[[\"q_no\", \"lang\", \"answer_number\", \"self_revision\", \"gen_answer\",\"revised_answer2\", \n",
    "                      \"is_correct\",\"is_correct2\",\"is_correct_rev\", \"is_correct_rev2\", \"ec_1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classified errors to a CSV file\n",
    "incorrect_df[[\"q_no\", \"lang\", \"answer_number\", \"gen_answer\",\"revised_answer2\", \"is_correct\",\n",
    "            \"is_correct2\",\"is_correct_rev\", \"is_correct_rev2\", \"ec_1\"]].to_csv(f'{folder}/checker.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for error categories for easier naming\n",
    "error_map = {\n",
    "    'Arithmetic Mistake': 'arithmetic',\n",
    "    'Missing Calculation': 'missing_calc',\n",
    "    'Hallucination': 'hallucination'\n",
    "}\n",
    "\n",
    "# List of error types to ensure consistent ordering\n",
    "error_types = list(error_map.keys())\n",
    "\n",
    "# Summarize the error counts per language\n",
    "summary = incorrect_df.groupby('lang')['ec_1'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Ensure all error columns are present\n",
    "for err in error_types:\n",
    "    if err not in summary.columns:\n",
    "        summary[err] = 0\n",
    "\n",
    "# Reorder and rename columns according to error_map\n",
    "summary = summary[error_types].reset_index()\n",
    "summary = summary.rename(columns=error_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group errors by type and language\n",
    "grouped_errors = {v: {} for v in error_map.values()}\n",
    "\n",
    "for _, row in incorrect_df.iterrows():\n",
    "    ec_raw = row['ec_1']\n",
    "    qno = row['q_no']\n",
    "    lang = row['lang']\n",
    "\n",
    "    if ec_raw in error_map:\n",
    "        err_type = error_map[ec_raw]\n",
    "\n",
    "        if lang not in grouped_errors[err_type]:\n",
    "            grouped_errors[err_type][lang] = []\n",
    "\n",
    "        grouped_errors[err_type][lang].append(qno)\n",
    "\n",
    "# Pretty print grouped errors by type and language\n",
    "from pprint import pprint\n",
    "pprint(grouped_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the grouped errors to a JSON file\n",
    "output_path = f\"{folder}/counts_error.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(grouped_errors, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved error summary to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlg804",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
